{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89fd4109-b555-46c7-a272-ab668da92fe2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5792185f-2bed-4124-8ba8-8947a11a9de9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件夹 ./model 已存在\n",
      "文件夹 ./tensor 已存在\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import tool\n",
    "import numpy as np\n",
    "\n",
    "Config = {\n",
    "    'device': \"cuda:1\" if torch.cuda.is_available() else \"cpu\",\n",
    "    'csv_file_path': 'data/test_right.csv',\n",
    "    'label_column': '微博主分类标注',\n",
    "\n",
    "    'Vectorizer_max_length': 140,\n",
    "    'Vectorizer_model_name': 'hfl/chinese-roberta-wwm-ext-large',\n",
    "\n",
    "    'train_ratio': 0.6,\n",
    "    'val_ratio': 0.2,\n",
    "    'batch_size': 64,\n",
    "\n",
    "    'epochs':6,\n",
    "    'model_save_path': 'similar-最后3分类.pth'\n",
    "}\n",
    "\n",
    "\n",
    "folders_to_create = ['./model', './tensor']\n",
    "for folder in folders_to_create:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    else:\n",
    "        print(f'文件夹 {folder} 已存在')\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df2326-24fe-4081-b43d-53c957a4a71a",
   "metadata": {},
   "source": [
    "# 数据预处理(数据量大小)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa04f717",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小数据数量：1299\n",
      "大数据数量：10780\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "csv = tool.CSVProcessor(Config['csv_file_path'])\n",
    "values= ['明星红人', '民主党派']\n",
    "csv.df = csv.df[~csv.df['微博主分类标注'].isin(values)]\n",
    "label_counts = csv.df['微博主分类标注'].value_counts()\n",
    "# print(label_counts)\n",
    "# print(\"=\" * 100)\n",
    "# 分大小数据\n",
    "threshold = 600\n",
    "csv.df['target_column'] = csv.df['微博主分类标注'].apply(lambda x: '小数据' if label_counts[x] < threshold else '大数据')\n",
    "small_data_count = (csv.df['target_column'] == '小数据').sum()\n",
    "large_data_count = (csv.df['target_column'] == '大数据').sum()\n",
    "\n",
    "print(f\"小数据数量：{small_data_count}\")\n",
    "print(f\"大数据数量：{large_data_count}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "values= ['小数据']\n",
    "csv.df = csv.df[~csv.df['target_column'].isin(values)]\n",
    "csv.df['target_column'] = csv.df['微博主分类标注']\n",
    "\n",
    "\n",
    "def map_labels(row):\n",
    "    if row['微博主分类标注'] in ['大V名人', '网民', '党委', '媒体', '自媒体', '企事业单位']:\n",
    "        return 'similar'\n",
    "    else:\n",
    "        return 'distinctive'\n",
    "    \n",
    "csv.df['target_column'] = csv.df.apply(map_labels, axis=1)\n",
    "\n",
    "valeus = ['distinctive', ]\n",
    "csv.df = csv.df[~csv.df['target_column'].isin(valeus)]\n",
    "csv.df['target_column'] = csv.df['微博主分类标注']\n",
    "\n",
    "valeus = ['党委','网民','自媒体']\n",
    "csv.df = csv.df[~csv.df['微博主分类标注'].isin(valeus)]\n",
    "\n",
    "# # conditions = [ csv.df['微博主分类标注'].isin(values) ]\n",
    "# # choices = ['其他']\n",
    "# # csv.df['target_column'] = np.select(conditions, choices, default=csv.df['微博主分类标注'])\n",
    "\n",
    "# label_counts = csv.df['微博主分类标注'].value_counts()\n",
    "# print(label_counts)\n",
    "# # values= ['网民', '自媒体']\n",
    "# # conditions = [ csv.df['微博主分类标注'].isin(values) ]\n",
    "# # choices = ['网自']\n",
    "# # csv.df['target_column'] = np.select(conditions, choices, default=csv.df['微博主分类标注'])\n",
    "# label_mapping = {'媒体': '其他',  '大V名人': '其他', '企事业单位':'其他',\n",
    "#                  '网民': '分出网友+自媒体', '自媒体': '分出网友+自媒体'}\n",
    "\n",
    "# 在新的一列中根据映射为数据添加新标签\n",
    "csv.df['target_column'] = csv.df['微博主分类标注']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1aadec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'大V名人': 0, '媒体': 1, '企事业单位': 2}\n",
      "3\n",
      "text:\n",
      "195      金V/002326/5313506/007832/母婴育儿博主 魏婷 美容专家 中央台评委 ...\n",
      "196      金V/003297/11895480/030788/微博2017十大影响力军事大V 知名军事...\n",
      "197      金V/000183/7420283/107015/知名美食博主 微博知名美食帐号000000...\n",
      "198      金V/000238/11989942/002004/演员，代表作《余罪》《追龙》《西虹市首富...\n",
      "199      金V/001798/9426455/029562/知名电视剧博主 电视剧视频自媒体00000...\n",
      "                               ...                        \n",
      "12065    蓝V/001589/932745/031263/小康杂志社官方微博0000000000000...\n",
      "12073    金V/000975/5360289/070568/微博知名搞笑博主0000000000000...\n",
      "12074    金V/001071/5008876/002515/BTV主持人悦悦0000000000000...\n",
      "12076    蓝V/000201/155537/009029/中国石油化工股份有限公司西北油田分公司官方微...\n",
      "12084    蓝V/000107/048572/001365/庐山西海国家级风景名胜区官方微博000000...\n",
      "Name: text, Length: 2669, dtype: object\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 列操作\n",
    "label_mapping = csv.generate_label_mapping('target_column')\n",
    "print(label_mapping)\n",
    "csv.label_numerization(label_mapping, 'target_column')  \n",
    "num_classes=len(label_mapping)\n",
    "print(num_classes)\n",
    "\n",
    "column_names = ['关注', '粉丝', '微博']\n",
    "for column_name in column_names:\n",
    "    csv.df[column_name] = csv.df[column_name].apply(lambda x: f'{int(x):06}' if pd.notnull(x) else '000000')\n",
    "csv.fill_nan_with_value()\n",
    "csv.df['认证'] = csv.df['认证'].replace('无', '无V')\n",
    "\n",
    "csv.str_length_normalization('博主标记',26)\n",
    "csv.str_length_normalization('简介',50)\n",
    "csv.str_length_normalization('工作信息',12)\n",
    "csv.str_length_normalization('标签和其他',25)\n",
    "\n",
    "all_to_merge = ['认证', '关注', '粉丝', '微博','博主标记', '简介', '工作信息', '标签和其他']\n",
    "csv.df['text'] = csv.apply_merge_to_columns(all_to_merge)\n",
    "\n",
    "text_column = csv.df['text']\n",
    "print('text:')\n",
    "print(text_column)\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a8d6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids_tensor: torch.Size([2669, 140])\n",
      "attention_mask_tensor: torch.Size([2669, 140])\n",
      "label_tensor: torch.Size([2669])\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 向量化\n",
    "tokenizer = tool.TextTokenizer(Config['Vectorizer_model_name'],\n",
    "                                Config['Vectorizer_max_length'])\n",
    "input_ids_list, attention_mask_list = tokenizer.tokenize_dataframe(csv.df['text'])\n",
    "label_list = list(csv.df['label_num'])\n",
    "input_ids_tensor = torch.tensor(input_ids_list)\n",
    "attention_mask_tensor = torch.tensor(attention_mask_list)\n",
    "label_tensor = torch.tensor(label_list)\n",
    "print('input_ids_tensor:',input_ids_tensor.shape)\n",
    "print('attention_mask_tensor:',attention_mask_tensor.shape)\n",
    "print('label_tensor:',label_tensor.shape)\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "841532c7-59a0-4d8a-91ea-ca498dc63b78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader检查:\n",
      "Batch 1:\n",
      "ids shape: torch.Size([64, 140])\n",
      "attention_mask shape: torch.Size([64, 140])\n",
      "Target shape: torch.Size([64])\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "td = tool.TrainDataset(input_ids_tensor, attention_mask_tensor, label_tensor)\n",
    "train_loader, val_loader, test_loader = td.prepare_dataloaders(Config['train_ratio'],                                                                                                                     \n",
    "                                                               Config['val_ratio'],\n",
    "                                                               Config['batch_size']\n",
    "                                                               )\n",
    "print('train_loader检查:')\n",
    "for batch_idx, (ids, attention_mask , target) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(\"ids shape:\", ids.shape)\n",
    "    print(\"attention_mask shape:\", attention_mask.shape)\n",
    "    print(\"Target shape:\", target.shape)\n",
    "\n",
    "    if batch_idx == 0:  \n",
    "        break\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccbcddd-d8e2-4a57-9156-2dd62904b147",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a164c39-3bb0-4b6d-a1ed-fe289c565f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext-large were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练过程:\n",
      "Epoch [1/6] Train Loss: 0.670032 Train Acc: 0.72 Val Loss: 3.854492 Val Acc: 0.35 Learning Rate: 0.010000\n",
      "\tTime: 00:30\n",
      "Epoch [2/6] Train Loss: 0.553901 Train Acc: 0.84 Val Loss: 1.854348 Val Acc: 0.64 Learning Rate: 0.009987\n",
      "\tTime: 00:38\n",
      "Epoch [3/6] Train Loss: 0.259350 Train Acc: 0.91 Val Loss: 0.093242 Val Acc: 0.97 Learning Rate: 0.009955\n",
      "\tTime: 00:38\n",
      "Epoch [4/6] Train Loss: 0.117857 Train Acc: 0.96 Val Loss: 0.098465 Val Acc: 0.98 Learning Rate: 0.009896\n",
      "\tTime: 00:39\n",
      "Epoch [5/6] Train Loss: 0.083267 Train Acc: 0.97 Val Loss: 0.281654 Val Acc: 0.92 Learning Rate: 0.009896\n",
      "\tTime: 00:28\n",
      "Epoch [6/6] Train Loss: 0.074207 Train Acc: 0.98 Val Loss: 0.145064 Val Acc: 0.96 Learning Rate: 0.009908\n",
      "\tTime: 00:28\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = tool.BERTVectorizer(Config['Vectorizer_model_name'], num_classes, Config['device'])\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-2,  weight_decay=5e-4)\n",
    "# optimizer=optim.RMSprop(model.parameters(),lr=0.001,alpha=0.99,momentum=0,weight_decay=0)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.5, weight_decay=5e-4)\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, Config['epochs'])\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,  'min',\n",
    "#                                                     factor=0.5, #学习率下降的因子factor=0.5, \n",
    "#                                                     verbose=True,#每次更新都会打印一条消息 \n",
    "#                                                     patience=2,#有2个epochs的平均损失没有变化，学习率将\n",
    "#                                                     min_lr=0.00000001,# 学习率的下限\n",
    "#                                                     threshold=0.001)#小于这个数表示平均损失没有下降\n",
    "# scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[75, 150], gamma=0.5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=15, T_mult=2, eta_min=0.00001, last_epoch=-1 , verbose=False)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print('训练过程:')\n",
    "trainer = tool.Trainer(model, train_loader, val_loader,optimizer,\n",
    "                          criterion, scheduler, Config['epochs'], Config['model_save_path'], Config['device'])\n",
    "trainer.train()\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cc3878-c924-4a1d-b268-133baf56c78c",
   "metadata": {},
   "source": [
    "# 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4f02419",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型测试:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext-large were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.954\n",
      "recall: 0.951\n",
      "F1: 0.951\n",
      "Accuracy of the network on the test items: 95.14 %\n",
      "Accuracy of  大V名人 : 98.03 %\n",
      "Accuracy of    媒体 : 97.88 %\n",
      "Accuracy of 企事业单位 : 90.21 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print('模型测试:')\n",
    "model = tool.BERTVectorizer(Config['Vectorizer_model_name'],num_classes, Config['device']) \n",
    "device = Config['device']\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('model/similar-最后3分类.pth'))\n",
    "model.eval()\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, labels in test_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "# 计算精确率、召回率和 F1 分数\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"recall: {recall:.3f}\")\n",
    "print(f\"F1: {f1:.3f}\")\n",
    "evaluator = tool.ModelEvaluator(model, test_loader, label_mapping, Config['device'])\n",
    "evaluator.test_accuracy()\n",
    "evaluator.accuracy_of_label()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8244e81-b35d-4617-a0d5-8f568881b28b",
   "metadata": {},
   "source": [
    "# 模型预测\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499e2c13-0b5d-4466-96ea-5b7cfd21dea7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '超话粉丝大咖', 1: '公务员', 2: '大V名人', 3: '党委', 4: '国防军委', 5: '基层组织', 6: '政府', 7: '检验检测', 8: '媒体', 9: '民主党派', 10: '明星红人', 11: '企事业单位', 12: '赛事活动', 13: '社会组织', 14: '社区组织', 15: '司法机关', 16: '外国政府机构', 17: '网民', 18: '行业专家', 19: '学校', 20: '研究机构', 21: '演艺娱乐明星', 22: '政协人大', 23: '自媒体'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import tool\n",
    "import tqdm\n",
    "\n",
    "\n",
    "Config_pdc = {\n",
    "    'csv_file_path': 'data/程序-头条1组链接.csv',\n",
    "    'num_classes': 24,\n",
    "    'device': \"cuda:1\" if torch.cuda.is_available() else \"cpu\",\n",
    "    'model_save_path': 'model/24class.pth',\n",
    "    'batch_size': 64,\n",
    "    'Vectorizer_max_length': 90,\n",
    "    'Vectorizer_model_name': 'hfl/chinese-roberta-wwm-ext-large'\n",
    "}\n",
    "\n",
    "original_dict = {'超话粉丝大咖': 0, '公务员': 1, '大V名人': 2, '党委': 3, '国防军委': 4, \n",
    "                 '基层组织': 5, '政府': 6, '检验检测': 7, '媒体': 8, '民主党派': 9, '明星红人': 10, \n",
    "                 '企事业单位': 11, '赛事活动': 12, '社会组织': 13, '社区组织': 14, '司法机关': 15,\n",
    "                 '外国政府机构': 16, '网民': 17, '行业专家': 18, '学校': 19, '研究机构': 20,\n",
    "                 '演艺娱乐明星': 21, '政协人大': 22, '自媒体': 23}\n",
    "reverse_dict = {value: key for key, value in original_dict.items()}\n",
    "print(reverse_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90fd1e5d-7e46-4c67-a2f2-99e432345765",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/guod/jupy/ding/tool.py:14: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.df= pd.read_csv(csv_file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          车快评00000000000000000/评论车市风云 点评汽车文化000000000000...\n",
      "1          车快评00000000000000000/评论车市风云 点评汽车文化000000000000...\n",
      "2          车快评00000000000000000/评论车市风云 点评汽车文化000000000000...\n",
      "3                                       勿燥.00000000000000000\n",
      "4                                       勿燥.00000000000000000\n",
      "                                 ...                        \n",
      "1006874                                 孤独的探路者00000000000000\n",
      "1006875                                 孤独的探路者00000000000000\n",
      "1006876                                 网事随风2900000000000000\n",
      "1006877                                 静看云起0000000000000000\n",
      "1006878                                 菩提青年感悟00000000000000\n",
      "Name: text, Length: 1006879, dtype: object\n"
     ]
    }
   ],
   "source": [
    "csv_pdc = tool.CSVProcessor(Config_pdc['csv_file_path'])\n",
    "csv_pdc.str_length_normalization('用户名',20)\n",
    "csv_pdc.str_length_normalization('用户认证',30)\n",
    "csv_pdc.str_length_normalization('认证信息',40)\n",
    "all_to_merge = ['认证信息', '用户名', '用户认证']\n",
    "csv_pdc.df['text'] = csv_pdc.apply_merge_to_columns(all_to_merge)\n",
    "print(csv_pdc.df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff4811b4-677b-4dfe-a836-0a1e07553472",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1806\n",
      "text:\n",
      "2565     无V/000167/000092/009016/超话粉丝大咖（蔡徐坤超话）000000000...\n",
      "2566     金V/000450/000203/006766/超话粉丝大咖（王晰超话）0000000000...\n",
      "2567     无V/002485/000731/006925/你看见了这个样子的世界 你要对善良的人好一点...\n",
      "2568     金V/000247/000600/007034/财经博主000000000000000000...\n",
      "2569     金V/000570/000209/033078/超话粉丝大咖（易烊千玺超话）00000000...\n",
      "                               ...                        \n",
      "12071    无V/000222/007977/000151/北京女神范文化传媒有限公司 平面模特0000...\n",
      "12075    无V/000380/004579/000482/视频自媒体00000000000000000...\n",
      "12079    金V/000112/841975/003449/北京协和医院营养科主治医师 李宁 健康博主0...\n",
      "12081    无V/000458/005589/003409/著名电影、电视剧观众。00000000000...\n",
      "12083    无V/000200/008303/000135/8。23。24000000000000000...\n",
      "Name: text, Length: 1806, dtype: object\n",
      "1806\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "csv_pdc = tool.CSVProcessor(Config_pdc['csv_file_path'])\n",
    "values= ['网民', '自媒体']\n",
    "csv_pdc.df = csv_pdc.df[csv_pdc.df['微博主分类标注'].isin(values)]\n",
    "print(csv_pdc.df.shape[0])\n",
    "\n",
    "column_names = ['关注', '粉丝', '微博']\n",
    "for column_name in column_names:\n",
    "    csv_pdc.df[column_name] = csv_pdc.df[column_name].apply(lambda x: f'{int(x):06}' if pd.notnull(x) else '000000')\n",
    "csv_pdc.fill_nan_with_value()\n",
    "csv_pdc.df['认证'] = csv_pdc.df['认证'].replace('无', '无V')\n",
    "\n",
    "csv_pdc.str_length_normalization('博主标记',26)\n",
    "csv_pdc.str_length_normalization('简介',50)\n",
    "csv_pdc.str_length_normalization('工作信息',12)\n",
    "csv_pdc.str_length_normalization('标签和其他',25)\n",
    "\n",
    "all_to_merge = ['认证', '关注', '粉丝', '微博','博主标记', '简介', '工作信息', '标签和其他']\n",
    "# all_to_merge = ['认证', '昵称']\n",
    "csv_pdc.df['text'] = csv_pdc.apply_merge_to_columns(all_to_merge)\n",
    "\n",
    "text_column = csv_pdc.df['text']\n",
    "print('text:')\n",
    "print(text_column)\n",
    "print(csv_pdc.df.shape[0])\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c920e-480b-4717-8116-ec4bc3afb037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = tool.TextTokenizer(Config_pdc['Vectorizer_model_name'],\n",
    "                                Config_pdc['Vectorizer_max_length'])\n",
    "input_ids_list, attention_mask_list = tokenizer.tokenize_dataframe(csv_pdc.df['text'])\n",
    "input_ids_tensor = torch.tensor(input_ids_list)\n",
    "attention_mask_tensor = torch.tensor(attention_mask_list)\n",
    "\n",
    "print('input_ids_tensor:',input_ids_tensor.shape)\n",
    "print('attention_mask_tensor:',attention_mask_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0c9af4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006879\n"
     ]
    }
   ],
   "source": [
    "pdc_dataset = tool.PredictionDataset(input_ids_tensor, attention_mask_tensor)\n",
    "pdc_dataloader = pdc_dataset.prepare_dataloader(64)\n",
    "\n",
    "model_pdc = tool.BERTVectorizer(Config_pdc['Vectorizer_model_name'],Config_pdc['num_classes'], Config_pdc['device']) \n",
    "result = tool.Prediction(model_pdc, Config_pdc['model_save_path'],Config_pdc['device'])\n",
    "predictions = []\n",
    "predictions = result.predict(pdc_dataloader)\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "804610e4-5266-488b-a8a7-46213f799269",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006879\n"
     ]
    }
   ],
   "source": [
    "predicted_label = [reverse_dict[number] for number in predictions]\n",
    "# print(predicted_label)\n",
    "print(csv_pdc.df.shape[0])\n",
    "csv_pdc.df['pdc'] = predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be38c55a-e264-4271-82d4-09709319a086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_pdc.df.to_csv('new_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "210800f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_pdc.df.to_csv('头条预测.csv',encoding='utf_8_sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "990a3524-642f-46f7-991e-6454a30740f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal rows ratio: 90.09%\n"
     ]
    }
   ],
   "source": [
    "csv_pdc.df.to_csv('new_dataframe.csv',  encoding='utf_8_sig', index=False)\n",
    "equal_rows = csv_pdc.df[csv_pdc.df['微博主分类标注'] == csv_pdc.df['pdc']]\n",
    "num_equal_rows = len(equal_rows)\n",
    "\n",
    "# 计算比重\n",
    "total_rows = len(csv_pdc.df)\n",
    "equal_rows_ratio = num_equal_rows / total_rows\n",
    "\n",
    "print(f\"Equal rows ratio: {equal_rows_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a82ebc8-42dc-45c7-9430-8d87c5c61efe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'网民': 0, '自媒体': 1}\n",
      "2\n",
      "Accuracy for class 0: 0.9392499810983139\n",
      "Accuracy for class 1: 0.939917695473251\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = predictions\n",
    "label_mapping = csv_pdc.generate_label_mapping('微博主分类标注')\n",
    "print(label_mapping)\n",
    "csv_pdc.label_numerization(label_mapping, '微博主分类标注')  \n",
    "num_classes=len(label_mapping)\n",
    "print(num_classes)\n",
    "true_labels = csv_pdc.df['label_num']\n",
    "class_0_predicted = [pred for pred, true in zip(predicted_labels, true_labels) if true == 0]\n",
    "class_1_predicted = [pred for pred, true in zip(predicted_labels, true_labels) if true == 1]\n",
    "\n",
    "# 计算每个类别的准确率\n",
    "accuracy_class_0 = sum([1 for pred, true in zip(class_0_predicted, true_labels) if pred == true]) / len(class_0_predicted)\n",
    "accuracy_class_1 = sum([1 for pred, true in zip(class_1_predicted, true_labels) if pred == true]) / len(class_1_predicted)\n",
    "\n",
    "print(\"Accuracy for class 0:\", accuracy_class_0)\n",
    "print(\"Accuracy for class 1:\", accuracy_class_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d98ac3a8-1605-4d36-a19b-c523e3ea200e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions for class 1: 3\n"
     ]
    }
   ],
   "source": [
    "# 假设你有模型的预测结果和真实标签\n",
    "predicted_labels = [1, 0, 1, 1, 0, 1, 0, 1, 0, 0]\n",
    "true_labels = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n",
    "\n",
    "# 统计原始标签为0且被正确预测的数量\n",
    "correct_predictions_for_class_0 = sum([1 for pred, true in zip(predicted_labels, true_labels) if true == 1 and pred == true])\n",
    "\n",
    "print(\"Correct predictions for class 1:\", correct_predictions_for_class_0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
